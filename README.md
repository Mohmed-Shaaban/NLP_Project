ğŸ“˜ NLP Preprocessing & N-Gram Language Model Project
Course Evaluation â€“ Text Normalization & Sentence Probability Calculation
â­ Project Overview

This project demonstrates essential NLP preprocessing techniques and applies N-gram language modeling (Markov assumption) to compute the probability of sentences from a dataset in CONLL-U format.

The workflow includes:

Dataset selection (UD English EWT)

Text preprocessing

Segmentation

Tokenization

Lowercasing

Stopword removal

Removing numbers & punctuation

Lemmatization

Building N-gram counts

Computing sentence probabilities

Displaying clean, well-formatted output

This project is designed to satisfy the first evaluation requirements for the NLP course.

ğŸ“‚ Dataset Used: UD English EWT (CONLLU format)

We use a well-known linguistic dataset: Universal Dependencies â€“ English Web Treebank (EWT).

Why choose this dataset?

âœ” Free and publicly available
âœ” Clean, annotated, linguistically valid
âœ” Contains high-quality segmentation, POS tags, lemmas, dependencies
âœ” Perfect for NLP preprocessing tasks
âœ” Lemmas are included â†’ helpful for normalization

Structure of Each Row (CONLL-U)
Column	Meaning
ID	Token index inside the sentence
FORM	Original word in the text
LEMMA	Normalized dictionary form
UPOS	Universal part of speech
XPOS	Language-specific part of speech
FEATS	Morphological features (Gender, Case, Number...)
HEAD	Governor (dependency parent)
DEPREL	Dependency relation type
DEPS	Enhanced dependencies
MISC	Extra metadata (SpaceAfter, alignmentâ€¦)
âš™ï¸ Installation & Setup
1ï¸âƒ£ Install required libraries
pip install nltk tqdm conllu

2ï¸âƒ£ Download NLTK resources
import nltk
nltk.download("stopwords")

ğŸ§¹ Preprocessing Steps (Explained)

Your code performs:

âœ“ 1. Token extraction

Reads each sentence from the CONLLU file.

âœ“ 2. Lowercasing

All text converted to lowercase.

âœ“ 3. Lemmatization

Using the LEMMA column (if available).

âœ“ 4. Removing:

Stopwords

Numbers

Punctuation

Empty tokens

Non-alphabetic symbols

âœ“ 5. Return clean, normalized tokens
ğŸ§® N-Gram Model (Markov Assumption)

Given a sentence:
I love natural language processing

The 2-gram probability is:

ğ‘ƒ
(
ğ‘†
)
=
ğ‘ƒ
(
ğ¼
)
Ã—
ğ‘ƒ
(
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
âˆ£
ğ¼
)
Ã—
ğ‘ƒ
(
ğ‘›
ğ‘
ğ‘¡
ğ‘¢
ğ‘Ÿ
ğ‘
ğ‘™
âˆ£
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
)
Ã—
ğ‘ƒ
(
ğ‘™
ğ‘
ğ‘›
ğ‘”
ğ‘¢
ğ‘
ğ‘”
ğ‘’
âˆ£
ğ‘›
ğ‘
ğ‘¡
ğ‘¢
ğ‘Ÿ
ğ‘
ğ‘™
)
Ã—
ğ‘ƒ
(
ğ‘
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘’
ğ‘ 
ğ‘ 
ğ‘–
ğ‘›
ğ‘”
âˆ£
ğ‘™
ğ‘
ğ‘›
ğ‘”
ğ‘¢
ğ‘
ğ‘”
ğ‘’
)
P(S)=P(I)Ã—P(loveâˆ£I)Ã—P(naturalâˆ£love)Ã—P(languageâˆ£natural)Ã—P(processingâˆ£language)

We compute:

N-gram counts

Conditional probabilities

Final probability per sentence

â–¶ï¸ Running the Project

Modify the file path:

conllu_path = "path/to/dataset.conllu"


Then run:

python main.py


The output will show:

ğŸŸ¦ Original Sentence
ğŸŸ© After Preprocessing
ğŸŸ§ N-gram Probability
ğŸ“¤ Output Example (Styled)
====================================
Sentence #1 (Original):
I really love learning NLP and I enjoy text processing.

Preprocessed:
love learn nlp enjoy text processing

2-Gram Probability:
1.2357e-12
====================================

ğŸ“Œ Project Files
File	Description
main.py	Contains the full preprocessing + N-gram probability calculation
README.md	Project documentation
dataset.conllu	The dataset used
ğŸ§‘â€ğŸ« Why This Project Is Important (Interview / Exam Points)

Shows understanding of text normalization pipelines

Demonstrates applying probability using N-gram models

Works with real linguistic datasets (CONLLU)

Uses lemmatization, which is more advanced than stemming

Shows ability to produce clean, structured output

Demonstrates practical NLP skills (preprocessing + modeling)

ğŸ™Œ Author

Mohamed â€” NLP Course Project
Faculty of Computers & Information
Mansoura University
